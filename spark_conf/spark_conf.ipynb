{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# spark conf\n",
    "\n",
    "- spark conf 종류 별로 확인 필요 \n",
    "- 기본적인 conf type 확인 \n",
    "    - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "\n",
    "\n",
    "db_path = 'd:/dw/data-warehouse/stocklab_db'\n",
    "appName = \"test_spark\"\n",
    "master = \"local[4]\"\n",
    "\n",
    "\n",
    "conf = SparkConf().setAll([(\"spark.driver.maxResultSize\", '6g'), ('spark.executor.cores', '1'), ('spark.submit.deployMode', 'client')])\n",
    "\n",
    "spark = SparkSession.builder.appName(appName) \\\n",
    "  .master(master) \\\n",
    "  .config(conf=conf) \\\n",
    "  .config(\"spark.sql.warehouse.dir\", db_path) \\\n",
    "  .config(\"spark.jars.packages\", \"io.delta:delta-core_2.12:0.8.0\") \\\n",
    "  .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "  .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "  .enableHiveSupport() \\\n",
    "  .getOrCreate()\n",
    "\n",
    "from delta.tables import *\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.app.startTime', '1621138930036'),\n",
       " ('spark.sql.catalog.spark_catalog',\n",
       "  'org.apache.spark.sql.delta.catalog.DeltaCatalog'),\n",
       " ('spark.files',\n",
       "  'file:///C:/Users/Administrator/.ivy2/jars/io.delta_delta-core_2.12-0.8.0.jar,file:///C:/Users/Administrator/.ivy2/jars/org.antlr_antlr4-4.7.jar,file:///C:/Users/Administrator/.ivy2/jars/org.antlr_antlr4-runtime-4.7.jar,file:///C:/Users/Administrator/.ivy2/jars/org.antlr_antlr-runtime-3.5.2.jar,file:///C:/Users/Administrator/.ivy2/jars/org.antlr_ST4-4.0.8.jar,file:///C:/Users/Administrator/.ivy2/jars/org.abego.treelayout_org.abego.treelayout.core-1.0.3.jar,file:///C:/Users/Administrator/.ivy2/jars/org.glassfish_javax.json-1.0.4.jar,file:///C:/Users/Administrator/.ivy2/jars/com.ibm.icu_icu4j-58.2.jar'),\n",
       " ('spark.app.initial.file.urls',\n",
       "  'file:///C:/Users/Administrator/.ivy2/jars/io.delta_delta-core_2.12-0.8.0.jar,file:///C:/Users/Administrator/.ivy2/jars/org.antlr_antlr-runtime-3.5.2.jar,file:///C:/Users/Administrator/.ivy2/jars/com.ibm.icu_icu4j-58.2.jar,file:///C:/Users/Administrator/.ivy2/jars/org.antlr_ST4-4.0.8.jar,file:///C:/Users/Administrator/.ivy2/jars/org.glassfish_javax.json-1.0.4.jar,file:///C:/Users/Administrator/.ivy2/jars/org.abego.treelayout_org.abego.treelayout.core-1.0.3.jar,file:///C:/Users/Administrator/.ivy2/jars/org.antlr_antlr4-4.7.jar,file:///C:/Users/Administrator/.ivy2/jars/org.antlr_antlr4-runtime-4.7.jar'),\n",
       " ('spark.master', 'local[4]'),\n",
       " ('spark.jars',\n",
       "  'file:///C:/Users/Administrator/.ivy2/jars/io.delta_delta-core_2.12-0.8.0.jar,file:///C:/Users/Administrator/.ivy2/jars/org.antlr_antlr4-4.7.jar,file:///C:/Users/Administrator/.ivy2/jars/org.antlr_antlr4-runtime-4.7.jar,file:///C:/Users/Administrator/.ivy2/jars/org.antlr_antlr-runtime-3.5.2.jar,file:///C:/Users/Administrator/.ivy2/jars/org.antlr_ST4-4.0.8.jar,file:///C:/Users/Administrator/.ivy2/jars/org.abego.treelayout_org.abego.treelayout.core-1.0.3.jar,file:///C:/Users/Administrator/.ivy2/jars/org.glassfish_javax.json-1.0.4.jar,file:///C:/Users/Administrator/.ivy2/jars/com.ibm.icu_icu4j-58.2.jar'),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.app.id', 'local-1621138933410'),\n",
       " ('spark.driver.host', '10.1.1.178'),\n",
       " ('spark.submit.pyFiles',\n",
       "  'C:/Users/Administrator/.ivy2/jars/io.delta_delta-core_2.12-0.8.0.jar,C:/Users/Administrator/.ivy2/jars/org.antlr_antlr4-4.7.jar,C:/Users/Administrator/.ivy2/jars/org.antlr_antlr4-runtime-4.7.jar,C:/Users/Administrator/.ivy2/jars/org.antlr_antlr-runtime-3.5.2.jar,C:/Users/Administrator/.ivy2/jars/org.antlr_ST4-4.0.8.jar,C:/Users/Administrator/.ivy2/jars/org.abego.treelayout_org.abego.treelayout.core-1.0.3.jar,C:/Users/Administrator/.ivy2/jars/org.glassfish_javax.json-1.0.4.jar,C:/Users/Administrator/.ivy2/jars/com.ibm.icu_icu4j-58.2.jar'),\n",
       " ('spark.sql.catalogImplementation', 'hive'),\n",
       " ('spark.sql.extensions', 'io.delta.sql.DeltaSparkSessionExtension'),\n",
       " ('spark.sql.warehouse.dir', 'd:/dw/data-warehouse/stocklab_db'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.driver.port', '63323'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.jars.packages', 'io.delta:delta-core_2.12:0.8.0'),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.app.initial.jar.urls',\n",
       "  'spark://10.1.1.178:63323/jars/org.antlr_antlr4-runtime-4.7.jar,spark://10.1.1.178:63323/jars/org.antlr_antlr4-4.7.jar,spark://10.1.1.178:63323/jars/io.delta_delta-core_2.12-0.8.0.jar,spark://10.1.1.178:63323/jars/com.ibm.icu_icu4j-58.2.jar,spark://10.1.1.178:63323/jars/org.antlr_ST4-4.0.8.jar,spark://10.1.1.178:63323/jars/org.abego.treelayout_org.abego.treelayout.core-1.0.3.jar,spark://10.1.1.178:63323/jars/org.glassfish_javax.json-1.0.4.jar,spark://10.1.1.178:63323/jars/org.antlr_antlr-runtime-3.5.2.jar'),\n",
       " ('spark.ui.showConsoleProgress', 'true'),\n",
       " ('spark.app.name', 'test_spark'),\n",
       " ('spark.repl.local.jars',\n",
       "  'file:///C:/Users/Administrator/.ivy2/jars/io.delta_delta-core_2.12-0.8.0.jar,file:///C:/Users/Administrator/.ivy2/jars/org.antlr_antlr4-4.7.jar,file:///C:/Users/Administrator/.ivy2/jars/org.antlr_antlr4-runtime-4.7.jar,file:///C:/Users/Administrator/.ivy2/jars/org.antlr_antlr-runtime-3.5.2.jar,file:///C:/Users/Administrator/.ivy2/jars/org.antlr_ST4-4.0.8.jar,file:///C:/Users/Administrator/.ivy2/jars/org.abego.treelayout_org.abego.treelayout.core-1.0.3.jar,file:///C:/Users/Administrator/.ivy2/jars/org.glassfish_javax.json-1.0.4.jar,file:///C:/Users/Administrator/.ivy2/jars/com.ibm.icu_icu4j-58.2.jar')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#spark.conf.set(\"spark.executor.memory\", '1g')\n",
    "#spark.conf.set('spark.executor.cores', '1')\n",
    "#spark.conf.set('spark.cores.max', '1')\n",
    "#spark.conf.set(\"spark.driver.memory\",'1g')\n",
    "# pyspark standalone 상에서는 spark conf 설정 시 아래 오류 발생 \n",
    "# AnalysisException: Cannot modify the value of a Spark config: spark.executor.memory\n",
    "\n",
    "sc = spark.sparkContext\n",
    "\n",
    "\n",
    "sc.getConf().getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "local[4]\n",
      "1\n",
      "cluster\n"
     ]
    }
   ],
   "source": [
    "print(spark.conf.get(\"spark.master\"))\n",
    "print( spark.conf.get(\"spark.executor.cores\"))\n",
    "\n",
    "print(spark.conf.get(\"spark.submit.deployMode\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Cannot modify the value of a Spark config: spark.submit.deployMode",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-87c638e5247b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# spark.conf.set('spark.executor.cores', '1')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'spark.submit.deployMode'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'cluster'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\spark\\lib\\site-packages\\pyspark\\sql\\conf.py\u001b[0m in \u001b[0;36mset\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m     34\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m         \u001b[1;34m\"\"\"Sets the given Spark runtime configuration property.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jconf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0msince\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2.0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\spark\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1302\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1304\u001b[1;33m         return_value = get_return_value(\n\u001b[0m\u001b[0;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0;32m   1306\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\spark\\lib\\site-packages\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    115\u001b[0m                 \u001b[1;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m                 \u001b[1;31m# JVM exception message.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 117\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    118\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m                 \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: Cannot modify the value of a Spark config: spark.submit.deployMode"
     ]
    }
   ],
   "source": [
    "# spark.conf.set('spark.executor.cores', '1')\n",
    "spark.conf.set('spark.submit.deployMode', 'cluster')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# config 변경을 통해 작업 진행이 가능한지 확인\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spark",
   "language": "python",
   "name": "spark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
