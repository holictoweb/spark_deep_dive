{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "spark_table_statistics_01.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMu8+kZagUJtVmQrcBJRP4r",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/holictoweb/spark_deep_dive/blob/master/spark_table/spark_table_statistics_01.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X5jEWkl8kutv",
        "outputId": "fb45031f-d72b-4da9-e0c3-3e4eeac91af7"
      },
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.conf import SparkConf\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.sql.functions import lit\n",
        " \n",
        "db_path = '/content/drive/MyDrive/data-warehouse/stocklab_db'\n",
        "\n",
        "spark = SparkSession.builder.appName('test_spark') \\\n",
        "  .config(\"spark.sql.warehouse.dir\", db_path) \\\n",
        "  .config(\"spark.jars.packages\", \"io.delta:delta-core_2.12:0.8.0\") \\\n",
        "  .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
        "  .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
        "  .enableHiveSupport() \\\n",
        "  .getOrCreate()\n",
        "\n",
        "from delta.tables import *\n",
        "from pyspark.sql.functions import *\n",
        "\n",
        "#spark.conf.set(\"spark.executor.memory\", '1g')\n",
        "#spark.conf.set('spark.executor.cores', '1')\n",
        "#spark.conf.set('spark.cores.max', '1')\n",
        "#spark.conf.set(\"spark.driver.memory\",'1g')\n",
        "sc = spark.sparkContext\n",
        "sc.getConf().getAll()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('spark.jars',\n",
              "  'file:///root/.ivy2/jars/io.delta_delta-core_2.12-0.8.0.jar,file:///root/.ivy2/jars/org.antlr_antlr4-4.7.jar,file:///root/.ivy2/jars/org.antlr_antlr4-runtime-4.7.jar,file:///root/.ivy2/jars/org.antlr_antlr-runtime-3.5.2.jar,file:///root/.ivy2/jars/org.antlr_ST4-4.0.8.jar,file:///root/.ivy2/jars/org.abego.treelayout_org.abego.treelayout.core-1.0.3.jar,file:///root/.ivy2/jars/org.glassfish_javax.json-1.0.4.jar,file:///root/.ivy2/jars/com.ibm.icu_icu4j-58.2.jar'),\n",
              " ('spark.driver.host', 'b94351e17554'),\n",
              " ('spark.driver.port', '39703'),\n",
              " ('spark.app.initial.jar.urls',\n",
              "  'spark://b94351e17554:39703/jars/org.abego.treelayout_org.abego.treelayout.core-1.0.3.jar,spark://b94351e17554:39703/jars/org.antlr_antlr4-runtime-4.7.jar,spark://b94351e17554:39703/jars/org.antlr_antlr-runtime-3.5.2.jar,spark://b94351e17554:39703/jars/org.antlr_antlr4-4.7.jar,spark://b94351e17554:39703/jars/org.glassfish_javax.json-1.0.4.jar,spark://b94351e17554:39703/jars/io.delta_delta-core_2.12-0.8.0.jar,spark://b94351e17554:39703/jars/com.ibm.icu_icu4j-58.2.jar,spark://b94351e17554:39703/jars/org.antlr_ST4-4.0.8.jar'),\n",
              " ('spark.app.id', 'local-1622781193015'),\n",
              " ('spark.executor.id', 'driver'),\n",
              " ('spark.repl.local.jars',\n",
              "  'file:///root/.ivy2/jars/io.delta_delta-core_2.12-0.8.0.jar,file:///root/.ivy2/jars/org.antlr_antlr4-4.7.jar,file:///root/.ivy2/jars/org.antlr_antlr4-runtime-4.7.jar,file:///root/.ivy2/jars/org.antlr_antlr-runtime-3.5.2.jar,file:///root/.ivy2/jars/org.antlr_ST4-4.0.8.jar,file:///root/.ivy2/jars/org.abego.treelayout_org.abego.treelayout.core-1.0.3.jar,file:///root/.ivy2/jars/org.glassfish_javax.json-1.0.4.jar,file:///root/.ivy2/jars/com.ibm.icu_icu4j-58.2.jar'),\n",
              " ('spark.app.initial.file.urls',\n",
              "  'file:///root/.ivy2/jars/org.antlr_ST4-4.0.8.jar,file:///root/.ivy2/jars/org.glassfish_javax.json-1.0.4.jar,file:///root/.ivy2/jars/org.antlr_antlr-runtime-3.5.2.jar,file:///root/.ivy2/jars/org.abego.treelayout_org.abego.treelayout.core-1.0.3.jar,file:///root/.ivy2/jars/io.delta_delta-core_2.12-0.8.0.jar,file:///root/.ivy2/jars/org.antlr_antlr4-runtime-4.7.jar,file:///root/.ivy2/jars/org.antlr_antlr4-4.7.jar,file:///root/.ivy2/jars/com.ibm.icu_icu4j-58.2.jar'),\n",
              " ('spark.sql.catalogImplementation', 'hive'),\n",
              " ('spark.sql.extensions', 'io.delta.sql.DeltaSparkSessionExtension'),\n",
              " ('spark.sql.warehouse.dir',\n",
              "  '/content/drive/MyDrive/data-warehouse/stocklab_db'),\n",
              " ('spark.rdd.compress', 'True'),\n",
              " ('spark.app.startTime', '1622781191540'),\n",
              " ('spark.serializer.objectStreamReset', '100'),\n",
              " ('spark.master', 'local[*]'),\n",
              " ('spark.jars.packages', 'io.delta:delta-core_2.12:0.8.0'),\n",
              " ('spark.submit.deployMode', 'client'),\n",
              " ('spark.files',\n",
              "  'file:///root/.ivy2/jars/io.delta_delta-core_2.12-0.8.0.jar,file:///root/.ivy2/jars/org.antlr_antlr4-4.7.jar,file:///root/.ivy2/jars/org.antlr_antlr4-runtime-4.7.jar,file:///root/.ivy2/jars/org.antlr_antlr-runtime-3.5.2.jar,file:///root/.ivy2/jars/org.antlr_ST4-4.0.8.jar,file:///root/.ivy2/jars/org.abego.treelayout_org.abego.treelayout.core-1.0.3.jar,file:///root/.ivy2/jars/org.glassfish_javax.json-1.0.4.jar,file:///root/.ivy2/jars/com.ibm.icu_icu4j-58.2.jar'),\n",
              " ('spark.ui.showConsoleProgress', 'true'),\n",
              " ('spark.submit.pyFiles',\n",
              "  '/root/.ivy2/jars/io.delta_delta-core_2.12-0.8.0.jar,/root/.ivy2/jars/org.antlr_antlr4-4.7.jar,/root/.ivy2/jars/org.antlr_antlr4-runtime-4.7.jar,/root/.ivy2/jars/org.antlr_antlr-runtime-3.5.2.jar,/root/.ivy2/jars/org.antlr_ST4-4.0.8.jar,/root/.ivy2/jars/org.abego.treelayout_org.abego.treelayout.core-1.0.3.jar,/root/.ivy2/jars/org.glassfish_javax.json-1.0.4.jar,/root/.ivy2/jars/com.ibm.icu_icu4j-58.2.jar'),\n",
              " ('spark.app.name', 'test_spark'),\n",
              " ('spark.sql.catalog.spark_catalog',\n",
              "  'org.apache.spark.sql.delta.catalog.DeltaCatalog')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WZ0OxN1XT8Kc",
        "outputId": "204a401c-d6c0-4ddb-80e7-a44bf7b8c8b7"
      },
      "source": [
        "print(spark.conf.get(\"spark.sql.inMemoryColumnarStorage.compressed\"))\n",
        "\n",
        "print(spark.conf.get(\"spark.sql.adaptive.enabled\"))\n",
        "print(spark.conf.get(\"spark.sql.adaptive.coalescePartitions.enabled\"))\n",
        "print(spark.conf.get(\"spark.sql.adaptive.coalescePartitions.minPartitionNum\"))\n",
        "\n",
        "print(spark.conf.get(\"spark.sql.hive.metastore.jars\"))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "true\n",
            "false\n",
            "true\n",
            "None\n",
            "builtin\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KjvfKuQWnQ6i",
        "outputId": "c1d68572-f333-4d7d-e5b7-95df5bd1b927"
      },
      "source": [
        "#spark.sql(\"drop database if exists stocklab CASCADE\")\n",
        "#AnalysisException: org.apache.hadoop.hive.ql.metadata.HiveException: InvalidOperationException(message:Database stocklab is not empty. One or more tables exist.)\n",
        "#force drop cascade\n",
        "spark.sql(\"create database IF NOT EXISTS stocklab\")\n",
        "spark.sql(\"use stocklab\")\n",
        "spark.sql(\"show databases \").show()\n",
        "spark.sql(\"describe database stocklab\").show(100,False)\n",
        "spark.sql(\"show tables\").show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---------+\n",
            "|namespace|\n",
            "+---------+\n",
            "|  default|\n",
            "| stocklab|\n",
            "+---------+\n",
            "\n",
            "+-------------+------------------------------------------------------------------+\n",
            "|info_name    |info_value                                                        |\n",
            "+-------------+------------------------------------------------------------------+\n",
            "|Database Name|stocklab                                                          |\n",
            "|Comment      |                                                                  |\n",
            "|Location     |file:/content/drive/MyDrive/data-warehouse/stocklab_db/stocklab.db|\n",
            "|Owner        |root                                                              |\n",
            "+-------------+------------------------------------------------------------------+\n",
            "\n",
            "+--------+---------+-----------+\n",
            "|database|tableName|isTemporary|\n",
            "+--------+---------+-----------+\n",
            "+--------+---------+-----------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jh7cj7rnmFLK"
      },
      "source": [
        "df_day = spark.read.format('parquet').load('/content/drive/MyDrive/data-warehouse/stock_day/Code=005930')\n",
        "#df_day.write.saveAsTable('stocklab.stock_day')\n",
        "#df_day.write.format(\"delta\").saveAsTable(\"stocklab.delta_stock_day\")\n",
        "df_day.createOrReplaceTempView(\"view_stock_day\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 817
        },
        "id": "HmlYu-vtpE9o",
        "outputId": "9c8112cc-693e-4b60-a8d7-2cc4998b87ec"
      },
      "source": [
        "spark.sql(\"show tables\").show()\n",
        "spark.sql(\"DESCRIBE  EXTENDED delta_stock_day \").show(truncate = False)\n",
        "spark.sql(\"ANALYZE TABLE delta_stock_day COMPUTE STATISTICS NOSCAN\").show(truncate = False)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------+---------------+-----------+\n",
            "|database|      tableName|isTemporary|\n",
            "+--------+---------------+-----------+\n",
            "|stocklab|delta_stock_day|      false|\n",
            "|stocklab|      stock_day|      false|\n",
            "+--------+---------------+-----------+\n",
            "\n",
            "+----------------------------+----------------------------------------------------------------------------------+-------+\n",
            "|col_name                    |data_type                                                                         |comment|\n",
            "+----------------------------+----------------------------------------------------------------------------------+-------+\n",
            "|Open                        |double                                                                            |       |\n",
            "|High                        |double                                                                            |       |\n",
            "|Low                         |double                                                                            |       |\n",
            "|Close                       |double                                                                            |       |\n",
            "|Volume                      |double                                                                            |       |\n",
            "|Code                        |string                                                                            |       |\n",
            "|                            |                                                                                  |       |\n",
            "|# Partitioning              |                                                                                  |       |\n",
            "|Not partitioned             |                                                                                  |       |\n",
            "|                            |                                                                                  |       |\n",
            "|# Detailed Table Information|                                                                                  |       |\n",
            "|Name                        |stocklab.delta_stock_day                                                          |       |\n",
            "|Location                    |file:/content/drive/MyDrive/data-warehouse/stocklab_db/stocklab.db/delta_stock_day|       |\n",
            "|Provider                    |delta                                                                             |       |\n",
            "|Table Properties            |[Type=MANAGED,delta.minReaderVersion=1,delta.minWriterVersion=2]                  |       |\n",
            "+----------------------------+----------------------------------------------------------------------------------+-------+\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "AnalysisException",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-40-11de133b7c7b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"show tables\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"DESCRIBE  EXTENDED delta_stock_day \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ANALYZE TABLE delta_stock_day COMPUTE STATISTICS NOSCAN\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36msql\u001b[0;34m(self, sqlQuery)\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'row1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'row2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'row3'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    722\u001b[0m         \"\"\"\n\u001b[0;32m--> 723\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wrapped\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    724\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    725\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtableName\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1305\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1307\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAnalysisException\u001b[0m: ANALYZE TABLE is not supported for v2 tables."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WKDWwloHvnHF"
      },
      "source": [
        "## Analyze table and columns\n",
        "https://docs.databricks.com/spark/latest/spark-sql/language-manual/sql-ref-syntax-aux-describe-table.html\n",
        "\n",
        "\n",
        "- temp view에는 적용이 불가능.\n",
        "- "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KNZAkP0Sp36K",
        "outputId": "60d02f9e-5ba1-4920-987c-049592e0153b"
      },
      "source": [
        "spark.sql(\"DESCRIBE  EXTENDED stock_day \").show(truncate = False)\n",
        "spark.sql(\"ANALYZE TABLE stock_day COMPUTE STATISTICS NOSCAN\").show(truncate = False)\n",
        "spark.sql(\"DESCRIBE  EXTENDED stock_day \").show(truncate = False)\n",
        "spark.sql(\"ANALYZE TABLE stock_day COMPUTE STATISTICS\").show(truncate = False)\n",
        "spark.sql(\"DESCRIBE  EXTENDED stock_day \").show(truncate = False)\n",
        "spark.sql(\"ANALYZE TABLE stock_day COMPUTE STATISTICS FOR COLUMNS code\").show(truncate = False)\n",
        "# ANALYZE TABLE table_name COMPUTE STATISTICS FOR COLUMNS column_name [column_name,\n",
        "spark.sql(\"DESCRIBE  EXTENDED stock_day code \").show(truncate = False)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----------------------------+----------------------------------------------------------------------------+-------+\n",
            "|col_name                    |data_type                                                                   |comment|\n",
            "+----------------------------+----------------------------------------------------------------------------+-------+\n",
            "|Open                        |double                                                                      |null   |\n",
            "|High                        |double                                                                      |null   |\n",
            "|Low                         |double                                                                      |null   |\n",
            "|Close                       |double                                                                      |null   |\n",
            "|Volume                      |double                                                                      |null   |\n",
            "|Code                        |string                                                                      |null   |\n",
            "|                            |                                                                            |       |\n",
            "|# Detailed Table Information|                                                                            |       |\n",
            "|Database                    |stocklab                                                                    |       |\n",
            "|Table                       |stock_day                                                                   |       |\n",
            "|Owner                       |root                                                                        |       |\n",
            "|Created Time                |Fri Jun 04 01:50:17 UTC 2021                                                |       |\n",
            "|Last Access                 |UNKNOWN                                                                     |       |\n",
            "|Created By                  |Spark 3.1.2                                                                 |       |\n",
            "|Type                        |MANAGED                                                                     |       |\n",
            "|Provider                    |parquet                                                                     |       |\n",
            "|Statistics                  |13794 bytes                                                                 |       |\n",
            "|Location                    |file:/content/drive/MyDrive/data-warehouse/stocklab_db/stocklab.db/stock_day|       |\n",
            "|Serde Library               |org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe                 |       |\n",
            "|InputFormat                 |org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat               |       |\n",
            "+----------------------------+----------------------------------------------------------------------------+-------+\n",
            "only showing top 20 rows\n",
            "\n",
            "++\n",
            "||\n",
            "++\n",
            "++\n",
            "\n",
            "+----------------------------+----------------------------------------------------------------------------+-------+\n",
            "|col_name                    |data_type                                                                   |comment|\n",
            "+----------------------------+----------------------------------------------------------------------------+-------+\n",
            "|Open                        |double                                                                      |null   |\n",
            "|High                        |double                                                                      |null   |\n",
            "|Low                         |double                                                                      |null   |\n",
            "|Close                       |double                                                                      |null   |\n",
            "|Volume                      |double                                                                      |null   |\n",
            "|Code                        |string                                                                      |null   |\n",
            "|                            |                                                                            |       |\n",
            "|# Detailed Table Information|                                                                            |       |\n",
            "|Database                    |stocklab                                                                    |       |\n",
            "|Table                       |stock_day                                                                   |       |\n",
            "|Owner                       |root                                                                        |       |\n",
            "|Created Time                |Fri Jun 04 01:50:17 UTC 2021                                                |       |\n",
            "|Last Access                 |UNKNOWN                                                                     |       |\n",
            "|Created By                  |Spark 3.1.2                                                                 |       |\n",
            "|Type                        |MANAGED                                                                     |       |\n",
            "|Provider                    |parquet                                                                     |       |\n",
            "|Statistics                  |13794 bytes                                                                 |       |\n",
            "|Location                    |file:/content/drive/MyDrive/data-warehouse/stocklab_db/stocklab.db/stock_day|       |\n",
            "|Serde Library               |org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe                 |       |\n",
            "|InputFormat                 |org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat               |       |\n",
            "+----------------------------+----------------------------------------------------------------------------+-------+\n",
            "only showing top 20 rows\n",
            "\n",
            "++\n",
            "||\n",
            "++\n",
            "++\n",
            "\n",
            "+----------------------------+----------------------------------------------------------------------------+-------+\n",
            "|col_name                    |data_type                                                                   |comment|\n",
            "+----------------------------+----------------------------------------------------------------------------+-------+\n",
            "|Open                        |double                                                                      |null   |\n",
            "|High                        |double                                                                      |null   |\n",
            "|Low                         |double                                                                      |null   |\n",
            "|Close                       |double                                                                      |null   |\n",
            "|Volume                      |double                                                                      |null   |\n",
            "|Code                        |string                                                                      |null   |\n",
            "|                            |                                                                            |       |\n",
            "|# Detailed Table Information|                                                                            |       |\n",
            "|Database                    |stocklab                                                                    |       |\n",
            "|Table                       |stock_day                                                                   |       |\n",
            "|Owner                       |root                                                                        |       |\n",
            "|Created Time                |Fri Jun 04 01:50:17 UTC 2021                                                |       |\n",
            "|Last Access                 |UNKNOWN                                                                     |       |\n",
            "|Created By                  |Spark 3.1.2                                                                 |       |\n",
            "|Type                        |MANAGED                                                                     |       |\n",
            "|Provider                    |parquet                                                                     |       |\n",
            "|Statistics                  |13794 bytes, 1506 rows                                                      |       |\n",
            "|Location                    |file:/content/drive/MyDrive/data-warehouse/stocklab_db/stocklab.db/stock_day|       |\n",
            "|Serde Library               |org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe                 |       |\n",
            "|InputFormat                 |org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat               |       |\n",
            "+----------------------------+----------------------------------------------------------------------------+-------+\n",
            "only showing top 20 rows\n",
            "\n",
            "++\n",
            "||\n",
            "++\n",
            "++\n",
            "\n",
            "+--------------+----------+\n",
            "|info_name     |info_value|\n",
            "+--------------+----------+\n",
            "|col_name      |code      |\n",
            "|data_type     |string    |\n",
            "|comment       |NULL      |\n",
            "|min           |NULL      |\n",
            "|max           |NULL      |\n",
            "|num_nulls     |0         |\n",
            "|distinct_count|1         |\n",
            "|avg_col_len   |6         |\n",
            "|max_col_len   |6         |\n",
            "|histogram     |NULL      |\n",
            "+--------------+----------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "heGYA1ZnuGoJ",
        "outputId": "90d2e1bd-fe32-427d-9bfe-fc064551e046"
      },
      "source": [
        "#spark.sql(\"ANALYZE TABLE stock_day COMPUTE STATISTICS FOR ALL COLUMNS\")\n",
        "spark.sql(\"DESCRIBE  EXTENDED stock_day \").show(truncate = False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----------------------------+----------------------------------------------------------------------------+-------+\n",
            "|col_name                    |data_type                                                                   |comment|\n",
            "+----------------------------+----------------------------------------------------------------------------+-------+\n",
            "|Open                        |double                                                                      |null   |\n",
            "|High                        |double                                                                      |null   |\n",
            "|Low                         |double                                                                      |null   |\n",
            "|Close                       |double                                                                      |null   |\n",
            "|Volume                      |double                                                                      |null   |\n",
            "|Code                        |string                                                                      |null   |\n",
            "|                            |                                                                            |       |\n",
            "|# Detailed Table Information|                                                                            |       |\n",
            "|Database                    |stocklab                                                                    |       |\n",
            "|Table                       |stock_day                                                                   |       |\n",
            "|Owner                       |root                                                                        |       |\n",
            "|Created Time                |Fri Jun 04 01:50:17 UTC 2021                                                |       |\n",
            "|Last Access                 |UNKNOWN                                                                     |       |\n",
            "|Created By                  |Spark 3.1.2                                                                 |       |\n",
            "|Type                        |MANAGED                                                                     |       |\n",
            "|Provider                    |parquet                                                                     |       |\n",
            "|Statistics                  |13794 bytes, 1506 rows                                                      |       |\n",
            "|Location                    |file:/content/drive/MyDrive/data-warehouse/stocklab_db/stocklab.db/stock_day|       |\n",
            "|Serde Library               |org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe                 |       |\n",
            "|InputFormat                 |org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat               |       |\n",
            "+----------------------------+----------------------------------------------------------------------------+-------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TyiJBTsCTWnR"
      },
      "source": [
        "## spark statistics 성능 비교 \n",
        "\n",
        "관련한 성능 "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jpkbV2Gsk5gP"
      },
      "source": [
        "## pyspark 사용방법\n",
        "- pip install pyspark 설치 이후 \n",
        "  stored in directory 위치 확인 \n",
        "  \n",
        "/root/.cache/pip/wheels/0b/90/c0/01de724414ef122bd05f056541fb6a0ecf47c7ca655f8b3c0f\n",
        "\n",
        "- spark home 지정\n",
        "\n",
        "PYSPARK_PYTHON=python3 SPARK_HOME=~/root/.cache/pip/wheels/0b/90/c0/01de724414ef122bd05f056541fb6a0ecf47c7ca655f8b3c0f"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vJu_-LbTk5IJ",
        "outputId": "bdec14b9-60a3-4036-fd7d-7f0d9b583eec"
      },
      "source": [
        "# 기본 설정 \n",
        "\n",
        "!pip install pyspark\n",
        "!pip install yfinance\n",
        "\n",
        "#!PYSPARK_PYTHON=python3 SPARK_HOME=~/root/.cache/pip/wheels/0b/90/c0/01de724414ef122bd05f056541fb6a0ecf47c7ca655f8b3c0f\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "!ls /content/drive/MyDrive/data-warehouse/stock_day\n",
        "\n",
        "#4/1AY0e-g4gHCUKl2LbLgrZScXnG9-_e0Pge08syfCh0dnVO_4oAW7VLqGDMYc"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pyspark\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/89/db/e18cfd78e408de957821ec5ca56de1250645b05f8523d169803d8df35a64/pyspark-3.1.2.tar.gz (212.4MB)\n",
            "\u001b[K     |████████████████████████████████| 212.4MB 53kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.9\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/b6/6a4fb90cd235dc8e265a6a2067f2a2c99f0d91787f06aca4bcf7c23f3f80/py4j-0.10.9-py2.py3-none-any.whl (198kB)\n",
            "\u001b[K     |████████████████████████████████| 204kB 16.7MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.1.2-py2.py3-none-any.whl size=212880768 sha256=494f1eab581761ae6943499e73fdc9de3f8eafeb9a153be7ff43d56e2e37e894\n",
            "  Stored in directory: /root/.cache/pip/wheels/40/1b/2c/30f43be2627857ab80062bef1527c0128f7b4070b6b2d02139\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9 pyspark-3.1.2\n",
            "Collecting yfinance\n",
            "  Downloading https://files.pythonhosted.org/packages/a7/ee/315752b9ef281ba83c62aa7ec2e2074f85223da6e7e74efb4d3e11c0f510/yfinance-0.1.59.tar.gz\n",
            "Requirement already satisfied: pandas>=0.24 in /usr/local/lib/python3.7/dist-packages (from yfinance) (1.1.5)\n",
            "Requirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.7/dist-packages (from yfinance) (1.19.5)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.7/dist-packages (from yfinance) (2.23.0)\n",
            "Requirement already satisfied: multitasking>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from yfinance) (0.0.9)\n",
            "Collecting lxml>=4.5.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/30/c0/d0526314971fc661b083ab135747dc68446a3022686da8c16d25fcf6ef07/lxml-4.6.3-cp37-cp37m-manylinux2014_x86_64.whl (6.3MB)\n",
            "\u001b[K     |████████████████████████████████| 6.3MB 2.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24->yfinance) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24->yfinance) (2018.9)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20->yfinance) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20->yfinance) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20->yfinance) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20->yfinance) (2020.12.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=0.24->yfinance) (1.15.0)\n",
            "Building wheels for collected packages: yfinance\n",
            "  Building wheel for yfinance (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for yfinance: filename=yfinance-0.1.59-py2.py3-none-any.whl size=23455 sha256=3043c1ba15aa12b16ebcb02267180ed40bc7635682b6849b3a2a890e56fb7a8e\n",
            "  Stored in directory: /root/.cache/pip/wheels/f8/2a/0f/4b5a86e1d52e451757eb6bc17fd899629f0925c777741b6d04\n",
            "Successfully built yfinance\n",
            "Installing collected packages: lxml, yfinance\n",
            "  Found existing installation: lxml 4.2.6\n",
            "    Uninstalling lxml-4.2.6:\n",
            "      Successfully uninstalled lxml-4.2.6\n",
            "Successfully installed lxml-4.6.3 yfinance-0.1.59\n",
            "Mounted at /content/drive\n",
            "'Code=005930'  'Code=035420'  'Code=064260'  'Code=068270'\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}